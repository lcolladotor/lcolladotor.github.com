<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JHU Biostat | L. Collado-Torres</title>
    <link>https://lcolladotor.github.io/categories/jhu-biostat/</link>
      <atom:link href="https://lcolladotor.github.io/categories/jhu-biostat/index.xml" rel="self" type="application/rss+xml" />
    <description>JHU Biostat</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2011-2020. All thoughts and opinions here are my own. The icon was designed by [Mauricio Guzmán](https://www.linkedin.com/in/mauricio-guzman-6529b551/) and is inspired by [Huichol culture](https://en.wikipedia.org/wiki/Huichol); it represents my community building interests</copyright><lastBuildDate>Wed, 20 Nov 2013 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://lcolladotor.github.io/images/icon_hu2b32c4ab415f12f472f73c7b0301b0d1_19400_512x512_fill_lanczos_center_2.png</url>
      <title>JHU Biostat</title>
      <link>https://lcolladotor.github.io/categories/jhu-biostat/</link>
    </image>
    
    <item>
      <title>Third Student Cultural Mixer</title>
      <link>https://lcolladotor.github.io/2013/11/20/studentmixer3/</link>
      <pubDate>Wed, 20 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2013/11/20/studentmixer3/</guid>
      <description>&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-20-StudentMixer3/ad-03_v1.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-20-StudentMixer3/ad-03_v1.png&#34; alt=&#34;Workflow&#34; style=&#34;width: 612px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;Note: the Fall event will be student and post doc only while the Spring event will be open to the whole department. This is to smooth the integration process for the new students. A survey showed that at least some first year students would be more likely to either attend or present if it&amp;rsquo;s a student-only event.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My strategy for quickly grading homeworks</title>
      <link>https://lcolladotor.github.io/2013/11/08/fast-grading/</link>
      <pubDate>Fri, 08 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2013/11/08/fast-grading/</guid>
      <description>&lt;p&gt;For the past year and a half 
&lt;a href=&#34;http://www.biostat.jhsph.edu/~lcollado/teaching.html#.Un2BnJTF2Ql&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;I have been a teaching assistant&lt;/a&gt; (TA) for the Statistical Methods in Public Health I to IV (140.621 to 140.624) courses. As part of being a TA for these courses, we have to grade between 30 and 50 homeworks every two weeks or so: four problem sets per eight week terms. For example, I now have to grade 37 homeworks:&lt;/p&gt;
&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/list-to-grade.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/list-to-grade.png&#34; alt=&#34;HWs to grade&#34; style=&#34;width: 400px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;These courses have been polished over time and the instructors do a great job organizing the TA group. For every problem set that we have to grade, we are given a full answer key that is later published in the course site so all the students can access it. As TAs we also receive general grading comments from the instructor; typically a two page document describing the key points that we should expect the students to be able to answer.&lt;/p&gt;
&lt;p&gt;As part of the general grading comments, we are asked to give feedback beyond checkmarks such as &amp;ldquo;good job&amp;rdquo;. When the student completely misses the answer, we can simply refer them to the answer key: if they miss too many, we ask them to re-do the homework. Things get more complicated when the student misses part of the question and writing comments specific to that answer can get very time consuming. We can always use &amp;ldquo;More detailed discussion on results needed. See answer key.&amp;rdquo; but it can be nice for the student if we give them a hint on what they did wrong.&lt;/p&gt;
&lt;p&gt;As a student 
&lt;a href=&#34;https://twitter.com/jhubiostat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jhubiostat&lt;/a&gt; we agree to dedicate five hours per week to TA responsibilities. In the courses I have been a TA for, the responsibilities include getting familiar with the content, attending TA office hours (either general questions or STATA questions), attending the TA meetings, grading the exams, and grading the homeworks. It might sound like a lot, but I currently feel like I can it all done within the five hours per week. To do so, I obviously need to be efficient grading the homeworks.&lt;/p&gt;
&lt;p&gt;Here is how I do it.&lt;/p&gt;
&lt;h3 id=&#34;setup&#34;&gt;Setup&lt;/h3&gt;
&lt;p&gt;Programs used:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.adobe.com/products/acrobatpro.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adobe Acrobat Pro&lt;/a&gt; which I believe we can get through Hopkins for free.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.alfredapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alfred v2&lt;/a&gt; with 
&lt;a href=&#34;http://www.alfredapp.com/powerpack/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Powerpack&lt;/a&gt; purchased/installed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Why Adobe Acrobat?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Adobe Acrobat allows you to post &amp;ldquo;stamps&amp;rdquo;. It already includes a checkmark and a red &lt;strong&gt;X&lt;/strong&gt;. I also imported some simple images I made to stamp the homework as: satisfactory, unsatisfactory, or incomplete. We have to clearly mark one of these options at the top of the graded homeworks.&lt;/li&gt;
&lt;li&gt;Some of the PDF files we get are large, so I use the &lt;code&gt;Save as -&amp;gt; reduced size pdf&lt;/code&gt; tool from Adobe Acrobat to save space and make it easier for the course administrator to upload the graded homeworks to 
&lt;a href=&#34;http://courseplus.jhsph.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoursePlus&lt;/a&gt; (the tool the school uses for managing courses).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Why 
&lt;a href=&#34;http://www.alfredapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alfred v2&lt;/a&gt;?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For grading homeworks all I really need is a software that can remember my recent copy-pastes and make it easy for me to choose among them to paste them back. For example, 
&lt;a href=&#34;http://www.clipmenu.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ClipMenu&lt;/a&gt; can do the job.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.alfredapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alfred v2&lt;/a&gt; has a &lt;code&gt;Clipboard&lt;/code&gt; feature which does the above. Compared to 
&lt;a href=&#34;http://www.clipmenu.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ClipMenu&lt;/a&gt; it has the advantage of allowing you to search within the recent copy-pastes, making it easy to find the comment you want to paste.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you are on Windows or Linux, there must be some programs that have this functionality.&lt;/p&gt;
&lt;h3 id=&#34;grading-workflow&#34;&gt;Grading workflow&lt;/h3&gt;
&lt;h4 id=&#34;most-used-comment-good-job&#34;&gt;Most used comment: Good job!&lt;/h4&gt;
&lt;p&gt;Let me begin with a very simple case. Basically, you find a answer that is well done and you want to compliment the student for it.&lt;/p&gt;
&lt;p&gt;In the homework I am currently grading, the students have to compute the 95 percent confidence interval for the difference in two proportions using the formula:&lt;/p&gt;
&lt;p&gt;$$ \hat p_1 - \hat p_2 \pm 1.96 \times \sqrt{ \frac{ \hat p_1 (1 - \hat p_1)}{ n_1} + \frac{ \hat p_2 (1 - \hat p_2)}{ n_2} } $$&lt;/p&gt;
&lt;p&gt;which leads to (0.00142, 0.00798) with the data they are using.&lt;/p&gt;
&lt;p&gt;An anonymous student (I tried to choose examples that are not identifiable) wrote an answer that has very similar numbers to those we expect. Since they included the STATA output I can recognize that they used rounded numbers (oddly only for sample 1 but not sample 2) which lead the student to slightly different values. That is not a big deal and I think it deserves the &amp;ldquo;good job!&amp;rdquo; comment. So using the &amp;ldquo;stamps&amp;rdquo;, I added a checkmark. Then I typed &amp;ldquo;Good job!&amp;rdquo; and finally, I copied it so I can paste it the next time I want to add this comment.&lt;/p&gt;
&lt;p&gt;The image below shows the state at which I am copying the comment.&lt;/p&gt;
&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/goodjob1.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/goodjob1.png&#34; alt=&#34;Good job 1&#34; style=&#34;width: 500px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;Moving on, I later found another student that has the correct answer presented in a different format. If &amp;ldquo;Good job!&amp;rdquo; is the most recent comment on my clipboard, I can simply paste it. If it is not, then I can use 
&lt;a href=&#34;http://www.alfredapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alfred v2&lt;/a&gt; to show me my recent &amp;ldquo;copies&amp;rdquo; and select &amp;ldquo;Good job!&amp;rdquo; from the list. In this case it is under &lt;code&gt;cmd + 2&lt;/code&gt;; you can also use the arrows and the return key, or the mouse. Although note that 
&lt;a href=&#34;http://www.alfredapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alfred v2&lt;/a&gt; is designed in such a way that you only have to use the keyboard to gain efficiency.&lt;/p&gt;
&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/goodjob2.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/goodjob2.png&#34; alt=&#34;Good job 2&#34; style=&#34;width: 500px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;The end result is a green checkmark with the &amp;ldquo;Good job!&amp;rdquo; comment pasted into it. The next time I use &lt;code&gt;cmd + v&lt;/code&gt; (the regular paste shortcut in Mac), &amp;ldquo;Good job!&amp;rdquo; will be pasted. So you only really need to invoke 
&lt;a href=&#34;http://www.alfredapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alfred v2&lt;/a&gt; when you want to change comments.&lt;/p&gt;
&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/goodjob3.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/goodjob3.png&#34; alt=&#34;Good job 3&#34; style=&#34;width: 500px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;h4 id=&#34;detailed-comments-where-the-strategy-pays-off&#34;&gt;Detailed comments: where the strategy pays off!&lt;/h4&gt;
&lt;p&gt;You obviously do not all this setup just for pasting &amp;ldquo;Good job!&amp;rdquo; everywhere. However, when grading homeworks you will encounter much more complicated cases where you might need to write a short explanation, point the student to the answer key, etc. To save time you want to minimize having to type the long comments each time you need to. That is where this strategy pays its dividends.&lt;/p&gt;
&lt;p&gt;In this example, I first find a plot that looks very similar to what the students were asked to do. It has a couple of mistakes. First, the age-gender group categories are not labeled in the plot, which makes it impossible to know which group is which unless you know how &lt;em&gt;agegen&lt;/em&gt; is coded. Given that it is the first time students were asked to make such a detailed plot, I can expect other students to make the same mistake. So I copy that comment and add it to my clipboard history as shown below.&lt;/p&gt;
&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/ci1.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/ci1.png&#34; alt=&#34;CI 1&#34; style=&#34;width: 500px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;This particular plot has another problem in that the confidence bands look much smaller than expected. The estimates per se look comparable to the expected plot, so I suspect that the student either used the wrong standard errors or forgot to multiply them by 1.96. So I copy this comment separately and add it to my clipboard history. Maybe other students made the same mistake, maybe they did not.&lt;/p&gt;
&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/ci2.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/ci2.png&#34; alt=&#34;CI 2&#34; style=&#34;width: 500px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;Having copied both comments separately, I am now ready for cases where they forget to label the X axis, use the wrong confidence bands, or make both mistakes together.&lt;/p&gt;
&lt;p&gt;Later on, I find another homework that has labels on the X axis. So instead of typing my comment, I simply invoke 
&lt;a href=&#34;http://www.alfredapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alfred v2&lt;/a&gt; and search the appropriate comment as shown below.&lt;/p&gt;
&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/ci3.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/ci3.png&#34; alt=&#34;CI 3&#34; style=&#34;width: 500px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;If my comment list is getting long, using 
&lt;a href=&#34;http://www.alfredapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alfred v2&lt;/a&gt; I can easily search my clipboard history by typing keywords. In this particular case, I know that my comment had something to do with the &amp;ldquo;axis&amp;rdquo;, so typing it in I quickly find the comment I want to use.&lt;/p&gt;
&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/ci4.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/ci4.png&#34; alt=&#34;CI 4&#34; style=&#34;width: 500px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;And voila! The comment is pasted!&lt;/p&gt;
&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/ci5.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-08-fast-grading/ci5.png&#34; alt=&#34;CI 5&#34; style=&#34;width: 500px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;h3 id=&#34;alternatives&#34;&gt;Alternatives&lt;/h3&gt;
&lt;p&gt;An alternative implementation is to create a spreadsheet where you manually save the comments you are using while grading the homework. When you want to use them, you open the spreadsheet, select the comment, manually copy it, and then you are set to paste it. Using 
&lt;a href=&#34;http://www.alfredapp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alfred v2&lt;/a&gt; basically makes this a bit faster, but the key point is to save the comments in a way that you can re-use them and combine them if necessary.&lt;/p&gt;
&lt;p&gt;If you want more pointers on this alternative, ask 
&lt;a href=&#34;https://twitter.com/YennyWebbV&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@YennyWebbV&lt;/a&gt; who is an expert at it.&lt;/p&gt;
&lt;h3 id=&#34;wrap-up&#34;&gt;Wrap up&lt;/h3&gt;
&lt;p&gt;I hope that others find this strategy useful. In my case, I went from taking 9-11 hours to grade homeworks when I first began as a TA to grading them in 3-4 hours.&lt;/p&gt;
&lt;p&gt;If you are interested in reading more posts from students 
&lt;a href=&#34;https://twitter.com/jhubiostat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jhubiostat&lt;/a&gt; check out 
&lt;a href=&#34;http://bmorebiostat.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bmorebiostat.com&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p&gt;&amp;quot;Bmore Biostats is born!&amp;quot; by Leonardo Collado Torres at &lt;a href=&#34;http://t.co/CXVuni60gh&#34;&gt;http://t.co/CXVuni60gh&lt;/a&gt; &lt;a href=&#34;http://t.co/uIovRg1bAP&#34;&gt;pic.twitter.com/uIovRg1bAP&lt;/a&gt;&lt;/p&gt;&amp;mdash; bmorebiostats (@bmorebiostats) &lt;a href=&#34;https://twitter.com/bmorebiostats/statuses/398917966676258816&#34;&gt;November 8, 2013&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Bmore Biostats is born!</title>
      <link>https://lcolladotor.github.io/2013/11/07/bmorebiostats/</link>
      <pubDate>Thu, 07 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2013/11/07/bmorebiostats/</guid>
      <description>&lt;p&gt;In recent weeks, I have met with a group of students 
&lt;a href=&#34;https://twitter.com/jhubiostat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jhubiostat&lt;/a&gt; interested in blogging about their research, tutorials, pieces of R code, among plenty of other subjects. Within this group we had the idea to aggregate our blogs so it would be easier for others to follow us and to easily promote our own blogs to a much larger audience. Basically, do what 
&lt;a href=&#34;http://www.r-bloggers.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R-bloggers&lt;/a&gt; does but focused on blogs from students at Johns Hopkins Biostatistics. Ideally, once a student publishes a new post, our site would pick it up and promote it.&lt;/p&gt;
&lt;p&gt;A quick search revealed that making such a site was technically possible via several 
&lt;a href=&#34;http://wordpress.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WordPress&lt;/a&gt; plugins available. We initally tried doing so via a wordpress.com account and quickly noticed that we needed a self-hosted wordpress account.&lt;/p&gt;
&lt;h3 id=&#34;paperli&#34;&gt;paper.li&lt;/h3&gt;
&lt;p&gt;In the meantime, we also explored the possibility of creating a &lt;em&gt;paper&lt;/em&gt; using 
&lt;a href=&#34;http://paper.li/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper.li&lt;/a&gt; which resulted in the 
&lt;a href=&#34;bit.ly/BmoreBiostats&#34;&gt;BmoreBiostats paper&lt;/a&gt;. The free version allows you to select up to 25 RSS feeds (among other sources) as the source of new content that you either publish&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;in the morning and afternoon,&lt;/li&gt;
&lt;li&gt;daily,&lt;/li&gt;
&lt;li&gt;weekly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This platform offers several options for promoting new content which we implemented. It can create posts to Twitter (see below), Facebook and LinkedIn.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p&gt;New posts from students &lt;a href=&#34;https://twitter.com/jhubiostat&#34;&gt;@jhubiostat&lt;/a&gt; &lt;a href=&#34;http://t.co/jnHXs7qRPJ&#34;&gt;http://t.co/jnHXs7qRPJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; bmorebiostats (@bmorebiostats) &lt;a href=&#34;https://twitter.com/bmorebiostats/statuses/398439067689689088&#34;&gt;November 7, 2013&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;You can also embed it on a webpage just like 
&lt;a href=&#34;http://www.biostat.jhsph.edu/~lcollado/BmoreBiostats.html#.UnxMX5TF2Qn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;I did in my own website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The drawbacks are that customization is limited, specially if you do not like how it looks. It also does not provide you with a RSS feed. However, I did manage to get that to work via Facebook as you can notice 
&lt;a href=&#34;http://feeds.feedburner.com/BmoreBiostatsPaper&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We have also heard some comments about paper.li been associated with spammers. I agree with this if you set the paper to be published either twice per day or daily. Weekly wise, I do not see it as a spam generator. However, this is in contradiction with the original goal of promoting posts as soon as they are made, and &lt;strong&gt;only&lt;/strong&gt; when there are new posts.&lt;/p&gt;
&lt;p&gt;Nevertheless, in my opinion, the paper.li alternative can work well as a weekly summary of the posts. Something like the 
&lt;a href=&#34;http://simplystatistics.org/?s=sunday&amp;#43;data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sunday data/statistics link roundoup&lt;/a&gt;s at 
&lt;a href=&#34;http://simplystatistics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SimplyStatistics&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;planetakicom&#34;&gt;planetaki.com&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;http://biostat.jhsph.edu/~jmuschel/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John&lt;/a&gt; found this resource and implemented a 
&lt;a href=&#34;http://www.planetaki.com/bmorestattest#start&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quick test&lt;/a&gt;. It looks very simple, yet it includes full length posts and was very easy to setup. Note that the material there is deleted after 7 days since they assume that you check it more than once per week. We saw this as a potential negative feature, plus there is no native support for social media.&lt;/p&gt;
&lt;h3 id=&#34;bmorebiostatcom&#34;&gt;bmorebiostat.com&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;http://mandymejia.wordpress.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amanda&lt;/a&gt; and 
&lt;a href=&#34;http://jfortinbiostats.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jean-Philippe&lt;/a&gt; [thanks for trying out Wordpress__.com__!] figured out the WordPress solution, got us a hosted service and reserved the domain 
&lt;a href=&#34;http://bmorebiostat.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bmorebiostat.com&lt;/a&gt;. Then it was just a matter of choosing a plugin to do the job: we are using 
&lt;a href=&#34;http://wordpress.org/plugins/feedwordpress/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FeedWordPress&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We split the work and some tweaked the layout and added nice pictures of the 
&lt;a href=&#34;http://bmorebiostat.com/contributors/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;six founding blog authors&lt;/a&gt;. Thanks to what I learnt making 
&lt;a href=&#34;http://bit.ly/FellBit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fellgernon Bit&lt;/a&gt; I added the social features: 
&lt;a href=&#34;http://www.addthis.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AddThis&lt;/a&gt; Smart Layers, connections to social media, 
&lt;a href=&#34;http://disqus.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus&lt;/a&gt; comments (they will not be used much), and 
&lt;a href=&#34;http://feedburner.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FeedBurner&lt;/a&gt; RSS feeds.&lt;/p&gt;
&lt;p&gt;I like how the site looks and it fully achieves our initial goal. Yay!&lt;/p&gt;
&lt;p&gt;Currently, 
&lt;a href=&#34;bit.ly/BmoreBiostat&#34;&gt;BmoreBiostats&lt;/a&gt; is set up in a way that only the beginning of each post is shown. We can also set it up to contain full posts, but then the &lt;code&gt;R&lt;/code&gt; code highlighting needs to be polished out.&lt;/p&gt;
&lt;h3 id=&#34;current-implementation&#34;&gt;Current implementation&lt;/h3&gt;
&lt;p&gt;The full current workflow is illustrated below:&lt;/p&gt;
&lt;center&gt;
&lt;a href=&#34;http://lcolladotor.github.io/figs/2013-11-07-bmorebiostats/BmoreBiostats-schema.png&#34;&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-07-bmorebiostats/BmoreBiostats-schema.png&#34; alt=&#34;Workflow&#34; style=&#34;width: 900px;&#34;/&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;As a group we are now thinking of dropping the paper.li route. Well, the only option we might use is the weekly one. One strong argument in favor of dropping the paper.li route is that one site avoids any dilution given by having two. Furthermore, we do not want to be seen as spammers although some shameless self-promotion is not so bad either (something I learnt from 
&lt;a href=&#34;https://twitter.com/hspter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@hspter&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I guess that I am the only one still in favor of weekly summaries and using the feature of embeding the paper in a website (like 
&lt;a href=&#34;http://www.biostat.jhsph.edu/~lcollado/BmoreBiostats.html#.UnxMX5TF2Qn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;). To minimize the dilution, all the paper.li links point to 
&lt;a href=&#34;http://bmorebiostat.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bmorebiostat.com&lt;/a&gt;. You can also argue that there might be some interested only on the weekly summaries. Aka, we are just giving others options!&lt;/p&gt;
&lt;p&gt;Maybe I am just reluctant to delete the 
&lt;a href=&#34;bit.ly/BmoreBiostats&#34;&gt;BmoreBiostats paper&lt;/a&gt; so soon after I finally completed it. However, some of the work involved is not going down the drain since the social media accounts needed for the paper are now being used by 
&lt;a href=&#34;http://bmorebiostat.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bmorebiostat.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that we do not see much of a problem with the fact that 
&lt;a href=&#34;http://bmorebiostat.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bmorebiostat.com&lt;/a&gt; involves monthly fees as we are hoping to get some department support (cross fingers!).&lt;/p&gt;
&lt;h3 id=&#34;going-forward&#34;&gt;Going forward&lt;/h3&gt;
&lt;p&gt;We are very excited that we have implemented the aggregator of student blogs from Johns Hopkins Biostatistics. We believe that it will be helpful to others including prosprective students. Now that the aggregator is practically finished, we can now move unto writing exciting posts!&lt;/p&gt;
&lt;p&gt;If you are a student 
&lt;a href=&#34;https://twitter.com/jhubiostat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jhubiostat&lt;/a&gt; &amp;ndash;or a former student&amp;ndash; and you have a Biostatistics blog that you want to add to 
&lt;a href=&#34;http://bmorebiostat.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bmorebiostat.com&lt;/a&gt;, let us know! If you are a current student 
&lt;a href=&#34;https://twitter.com/jhubiostat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@jhubiostat&lt;/a&gt; and need help getting started, contact us!&lt;/p&gt;
&lt;p&gt;Be sure to follow us! Here is a hint why:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p&gt;It seems clear that &lt;a href=&#34;https://twitter.com/jhubiostat&#34;&gt;@jhubiostat&lt;/a&gt; is just dominating all the social media &lt;a href=&#34;https://twitter.com/bmorebiostats&#34;&gt;@bmorebiostats&lt;/a&gt;&lt;/p&gt;&amp;mdash; Simply Statistics (@simplystats) &lt;a href=&#34;https://twitter.com/simplystats/statuses/398625668851716096&#34;&gt;November 8, 2013&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Epi vs Biostat Kickball match Spring 2013</title>
      <link>https://lcolladotor.github.io/2013/04/24/epi-vs-biostat-kickball-match-spring-2013/</link>
      <pubDate>Wed, 24 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2013/04/24/epi-vs-biostat-kickball-match-spring-2013/</guid>
      <description>&lt;p&gt;This past Saturday the Epi and Biostat troops met for another fun kickball match. Obviously Biostat beat Epi, yup I know: again! This time the score was 15-8 (according to our bookkeeper and captain John) or 12-8 (according to some in Epi).&lt;/p&gt;
&lt;p&gt;There was a hint of a surprise at the beginning when Epi scored two runs in the top of the first inning. However, the tide changed back with a homerun by Rumen. Sadly, one of the Epi players got injured and carried out of the court in that play. Rumen also pulled his quad with the big hit and was limited for the rest of the match.&lt;/p&gt;
&lt;p&gt;From that inning on forth we saw both teams having fun kicking the ball as far as we could or aim for in between the defensive lines. There were plenty of sacrifice hits, some occasional errors, but overall we had a lot of fun!&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Both teams picture&#34; src=&#34;http://biostat.jhsph.edu/%7Elcollado/misc/Kickball2013/images/2013_04_20_16_55_15.jpg&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Both teams came prepared to show their colors: them in red us in purple with some face paint for the sport battle (thanks to Aaron). However, the Epi crew did surprise us by bringing a big grill to the park and lots of food!&lt;/p&gt;
&lt;p&gt;At the end of the match, we all mingled together and enjoyed the nice (a bit chilly) day outside in the company of some drinks and food.&lt;/p&gt;
&lt;p&gt;Some of us then continued our journey at Kislings where we played other games that involve loads of cups and some ping pong balls ;)&lt;/p&gt;
&lt;p&gt;You can &lt;a href=&#34;http://biostat.jhsph.edu/%7Elcollado/misc/Kickball2013/index.html&#34;&gt;view all the pictures here&lt;/a&gt;. If you have any other pictures that you want to share, send them my way!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting who will win a NFL match at half time</title>
      <link>https://lcolladotor.github.io/2013/03/23/predicting-who-will-win-a-nfl-match-at-half-time/</link>
      <pubDate>Sat, 23 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2013/03/23/predicting-who-will-win-a-nfl-match-at-half-time/</guid>
      <description>&lt;p&gt;It was great to have a little break, &lt;em&gt;Spring break&lt;/em&gt;, although the weather didn&amp;#8217;t feel like spring at all! During the early part of the break I worked on my final project for Jeff Leek&amp;#8217;s data analysis class, which we call 140.753 here. Continuing &lt;a href=&#34;http://fellgernon.tumblr.com/tagged/jhsph753#.UU44Y1vF2c4&#34;&gt;my previous posts on the topic&lt;/a&gt;, this time I&amp;#8217;ll share the results of my final project.&lt;/p&gt;
&lt;p&gt;At the beginning of the course, we had to submit a project plan (more like a proposal) and &lt;a href=&#34;https://github.com/lcolladotor/lcollado753/blob/master/hw/projectplan/lcollado_projectplan.pdf&#34;&gt;in mine&lt;/a&gt; I announced my interest to look into some sports data. At the time I included a few links to Brian Burke&amp;#8217;s Advanced NFL Stats site (&lt;span class=&#34;showtooltip&#34; title=&#34;(2013). Advanced NFL Stats.   http://www.advancednflstats.com/ [Online. last-accessed:  2013-03-23 23:28:38].  http://www.advancednflstats.com/.&#34;&gt;&lt;a href=&#34;http://www.advancednflstats.com/&#34;&gt;Burke&lt;/a&gt;&lt;/span&gt;). At the time I didn&amp;#8217;t know that Burke&amp;#8217;s site described in detail a lot of the information I would end up using.&lt;/p&gt;
&lt;p&gt;My final project had to do with splitting NFL games by half and then use only the play-by-play data from the first half to predict if team A or B would win the game. My overall goal was to have some fun with sports data which I had never looked at, but then also try to come up with something I would personally use in the future. So, why split games by half? I personally would like to know if I should keep watching a game or not at half time. Having a tool to help me decide would be great, and well, if the team I&amp;#8217;m rooting for has high chances of losing or winning, ideally I would switch to doing something else. A related question that I didn&amp;#8217;t try to answer is which half is worth watching? This would be a meaningful question if you only have time to watch one of them.&lt;/p&gt;
&lt;p&gt;To truly satisfy my goals, it wasn&amp;#8217;t enough to just build a predictive model. That is why I also built a web application using the &lt;code&gt;shiny&lt;/code&gt; package (&lt;span class=&#34;showtooltip&#34; title=&#34;RStudio and Inc. (2013). _shiny: Web Application Framework for R_.  R package version 0.4.0,   http://CRAN.R-project.org/package=shiny.&#34;&gt;&lt;a href=&#34;http://CRAN.R-project.org/package=shiny&#34;&gt;RStudio and Inc., 2013&lt;/a&gt;&lt;/span&gt;). It was the first time I did a shiny app, but thanks to the good manual and some examples on GitHub from John Muschelli like his &lt;a href=&#34;https://github.com/muschellij2/Shiny_model&#34;&gt;Shiny_model&lt;/a&gt; it wasn&amp;#8217;t so bad. I thus invite you to test and browse my shiny app at &lt;a href=&#34;http://glimmer.rstudio.com/lcolladotor/NFLhalf/&#34;&gt;&lt;a href=&#34;http://glimmer.rstudio.com/lcolladotor/NFLhalf/&#34;&gt;http://glimmer.rstudio.com/lcolladotor/NFLhalf/&lt;/a&gt;&lt;/a&gt;. It could be improved by adding some functions that scrape live data for the 2013 season so you don&amp;#8217;t have to input all the variables needed by using the sliders. Anyhow, I&amp;#8217;m happy with the result.&lt;/p&gt;
&lt;p&gt;The entire project&amp;#8217;s code, EDA steps, shiny app, and report are available via GitHub in my repository (&lt;span class=&#34;showtooltip&#34; title=&#34;lcolladotor (2013). lcollado753.   https://github.com/lcolladotor/lcollado753 [Online.  last-accessed: 2013-03-21 02:23:49].   https://github.com/lcolladotor/lcollado753/tree/master/final/nfl_half.&#34;&gt;&lt;a href=&#34;https://github.com/lcolladotor/lcollado753/tree/master/final/nfl_half&#34;&gt;lcollado753&lt;/a&gt;&lt;/span&gt;). While the details are in the report, I&amp;#8217;ll give a brief summary here.&lt;/p&gt;
&lt;p&gt;Basically, I summarized the play-by-play data for all NFL games from 2002 to 2012 seasons as provided by Burke (&lt;span class=&#34;showtooltip&#34; title=&#34;(2010). Advanced NFL Stats: Play-by-Play Data.   http://www.advancednflstats.com/2010/04/play-by-play-data.html  [Online. last-accessed: 2013-03-24 00:08:20].   http://www.advancednflstats.com/2010/04/play-by-play-data.html.&#34;&gt;&lt;a href=&#34;http://www.advancednflstats.com/2010/04/play-by-play-data.html&#34;&gt;Burke, 2010&lt;/a&gt;&lt;/span&gt;). I used some of the variables Burke uses (&lt;span class=&#34;showtooltip&#34; title=&#34;(2009). Advanced NFL Stats: How the Model Works-A Detailed  Example Part 1.   http://www.advancednflstats.com/2009/01/how-model-works-detailed-example.html  [Online. last-accessed: 2013-03-24 00:08:21].   http://www.advancednflstats.com/2009/01/how-model-works-detailed-example.html.&#34;&gt;&lt;a href=&#34;http://www.advancednflstats.com/2009/01/how-model-works-detailed-example.html&#34;&gt;Burke, 2009&lt;/a&gt;&lt;/span&gt;) and some others like the score difference, who starts the second half, and the game day winning percentages of both teams. After exploring the data, I discarded the years 2002 to 2005. Then, I trained a model using the 2006 to 2011 data and did some quick model selection. Note that I&amp;#8217;m not doing the adjustment by opponent the way Burke did it (&lt;span class=&#34;showtooltip&#34; title=&#34;(2009). Advanced NFL Stats: How the Model Works-A Detailed  Example Part 2.   http://www.advancednflstats.com/2009/01/how-model-works-detailed-example-part-2.html  [Online. last-accessed: 2013-03-24 00:08:23].   http://www.advancednflstats.com/2009/01/how-model-works-detailed-example-part-2.html.&#34;&gt;&lt;a href=&#34;http://www.advancednflstats.com/2009/01/how-model-works-detailed-example-part-2.html&#34;&gt;Burke, 2009-2&lt;/a&gt;&lt;/span&gt;) in part because I was running out of time, but also because the model already uses the current game winning percentages of both teams to consider the two team&amp;#8217;s strength. I evaluated the model using the 2012 data and after seeing that it worked decently enough, I trained a second model using the data from 2006 to 2012 so it can be used for the 2013 season. These two trained models are the ones available in the shiny app I made.&lt;/p&gt;
&lt;p&gt;In the report, I didn&amp;#8217;t include ROCs—a big miss—so here they go. The code I will show below is heavily based on a post on GLMs (&lt;span class=&#34;showtooltip&#34; title=&#34;denishaine (2013). Veterinary Epidemiologic Research: GLM  \ Evaluating Logistic Regression Models (part 3).   http://denishaine.wordpress.com/2013/03/19/veterinary-epidemiologic-research-glm-evaluating-logistic-regression-models-part-3/  [Online. last-accessed: 2013-03-23 22:51:49].   http://denishaine.wordpress.com/2013/03/19/veterinary-epidemiologic-research-glm-evaluating-logistic-regression-models-part-3/.&#34;&gt;&lt;a href=&#34;http://denishaine.wordpress.com/2013/03/19/veterinary-epidemiologic-research-glm-evaluating-logistic-regression-models-part-3/&#34;&gt;denishaine, 2013&lt;/a&gt;&lt;/span&gt;). The code below is written in a way that you can easily reproduce it if you have cloned my repository for the 140.753 class (&lt;span class=&#34;showtooltip&#34; title=&#34;lcolladotor (2013). lcollado753.   https://github.com/lcolladotor/lcollado753 [Online.  last-accessed: 2013-03-21 02:23:49].   https://github.com/lcolladotor/lcollado753/tree/master/final/nfl_half.&#34;&gt;&lt;a href=&#34;https://github.com/lcolladotor/lcollado753/tree/master/final/nfl_half&#34;&gt;lcollado753&lt;/a&gt;&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;First, some setup steps.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Specify the directory where you cloned the lcollado753 repo
maindir &amp;lt;- &amp;quot;whereYouClonedTheRepo&amp;quot;
## Load packages needed
suppressMessages(library(ROCR))
library(ggplot2)

## Load fits.
## Remember that 1st one used data from 2006 to 2011
## and the 2nd one used data from 2006 to 2012.
load(paste0(maindir, &amp;quot;/lcollado753/final/nfl_half/EDA/model/fits.Rdata&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I make the ROCs for both trained models using the data that they were trained on. They should be quite good since it uses the same data to build the model that it will then try to predict.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Make the ROC plots

## Simple list where I&#39;ll store all the results so I can compare the ROC plots later on
all &amp;lt;- list()

## Construct prediction function
for(i in 1:2) {
	## Predict on the original data
	pred &amp;lt;- predict(fits[[i]])
	
	## Subset original data (remove NA&#39;s)
	data &amp;lt;- fits[[i]]$data
	data &amp;lt;- data[complete.cases(data),]
	
	## Construct prediction function
	pred.fn &amp;lt;- prediction(pred, data$win)
	
	## Get performance info
	perform &amp;lt;- performance(pred.fn, &amp;quot;tpr&amp;quot;, &amp;quot;fpr&amp;quot;)
	
	## Get ready to plot
	toPlot &amp;lt;- data.frame(tpr = unlist(slot(perform, &amp;quot;y.values&amp;quot;)), fpr = unlist(slot(perform, &amp;quot;x.values&amp;quot;)))
	all &amp;lt;- c(all, list(toPlot))

	## Make the plot
	res &amp;lt;- ggplot(toPlot) + geom_line(aes(x=fpr, y=tpr)) + geom_abline(intercept=0, slope=1, colour=&amp;quot;orange&amp;quot;) + ylab(&amp;quot;Sensitivity&amp;quot;) + xlab(&amp;quot;1 - Specificity&amp;quot;) + ggtitle(paste(&amp;quot;Years 2006 to&amp;quot;, c(&amp;quot;2011&amp;quot;, &amp;quot;2012&amp;quot;)[i]))
	print(res)
	
	## Print the AUC value
	print(unlist(performance(pred.fn, &amp;quot;auc&amp;quot;)@y.values))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&#34;plot of chunk ROC&#34; src=&#34;http://i.imgur.com/b1FS2ml.png&#34;/&gt;&lt;/p&gt;
```r
## [1] 0.8506
```
&lt;p&gt;&lt;img alt=&#34;plot of chunk ROC&#34; src=&#34;http://i.imgur.com/f2UOySy.png&#34;/&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## [1] 0.8513
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both ROC plots look pretty similar (well, the data sets are very similar!) and have relatively high AUC values.&lt;/p&gt;
&lt;p&gt;Next, I make the ROC plot using the model trained with the data from 2006 to 2011 to predict the outcomes for the 2012 games.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Load 2012 data
load(paste0(maindir, &amp;quot;/lcollado753/final/nfl_half/data/pred/info2012.Rdata&amp;quot;))

## Predict using model fit with data from 2006 to 2011
pred &amp;lt;- predict(fits[[1]], info2012)

## Construction prediction function
pred.fn &amp;lt;- prediction(pred, info2012$win)

## Get performance info
perform &amp;lt;- performance(pred.fn, &amp;quot;tpr&amp;quot;, &amp;quot;fpr&amp;quot;)

## Get ready to plot
toPlot &amp;lt;- data.frame(tpr = unlist(slot(perform, &amp;quot;y.values&amp;quot;)), fpr = unlist(slot(perform, &amp;quot;x.values&amp;quot;)))
all &amp;lt;- c(all, list(toPlot))

## Make the plot
ggplot(toPlot) + geom_line(aes(x=fpr, y=tpr)) + geom_abline(intercept=0, slope=1, colour=&amp;quot;orange&amp;quot;) + ylab(&amp;quot;Sensitivity&amp;quot;) + xlab(&amp;quot;1 - Specificity&amp;quot;) + ggtitle(&amp;quot;Model trained 2006-2011 predicting 2012&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&#34;plot of chunk pred2012&#34; src=&#34;http://i.imgur.com/DDcsW7W.png&#34;/&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Print the AUC value
print(unlist(performance(pred.fn, &amp;quot;auc&amp;quot;)@y.values))
## [1] 0.816
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The steps in the curve are more visible since it is using less data. It also seems to be a little less good than the other two, as expected. This is clear when comparing the AUC values.&lt;/p&gt;
&lt;p&gt;Finally, I plot all curves in the same picture to visually compare them.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(all) &amp;lt;- c(&amp;quot;train2011&amp;quot;, &amp;quot;train2012&amp;quot;, &amp;quot;pred2012&amp;quot;)
for(i in 1:3) {
	all[[i]] &amp;lt;- cbind(all[[i]], rep(names(all)[i], nrow(all[[i]])))
	colnames(all[[i]])[3] &amp;lt;- &amp;quot;set&amp;quot;
}
all &amp;lt;- do.call(rbind, all)

ggplot(all) + geom_line(aes(x=fpr, y=tpr, colour=set)) + geom_abline(intercept=0, slope=1, colour=&amp;quot;orange&amp;quot;) + ylab(&amp;quot;Sensitivity&amp;quot;) + xlab(&amp;quot;1 - Specificity&amp;quot;) + ggtitle(&amp;quot;Comparing ROCs&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt=&#34;plot of chunk allInOne&#34; src=&#34;http://i.imgur.com/tUVfgfs.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Both ROCs with the trained data (train2011, train2012) are nearly identical and both are slightly superior to the one predicting the 2012 games.&lt;/p&gt;
&lt;p&gt;Overall I am happy with the results and while some things can certainly be improved, I look forward to the NFL 2013 season. Also, remember that Burke publishes his winning estimated probabilities from week 4 onward (&lt;span class=&#34;showtooltip&#34; title=&#34;BURKE BB (2013). Brian Burke - The Fifth Down Blog -  NYTimes.com.   http://fifthdown.blogs.nytimes.com/author/brian-burke/ [Online.  last-accessed: 2013-03-24 00:26:32].   http://fifthdown.blogs.nytimes.com/author/brian-burke/.&#34;&gt;&lt;a href=&#34;http://fifthdown.blogs.nytimes.com/author/brian-burke/&#34;&gt;The Fifth Down Blog&lt;/a&gt;&lt;/span&gt;). So you might be interested on comparing the probability at half time versus his estimated probability which is calculated before the game starts. I mean, maybe you could use the difference between the two to have an idea of how unexpected the first half was. After all, if a game falls outside the pattern it might be worth watching.&lt;/p&gt;
&lt;p&gt;Citations made with &lt;code&gt;knitcitations&lt;/code&gt; (&lt;span class=&#34;showtooltip&#34; title=&#34;Boettiger C (2013). _knitcitations: Citations for knitr markdown  files_. R package version 0.4-4,   https://github.com/cboettig/knitcitations.&#34;&gt;&lt;a href=&#34;https://github.com/cboettig/knitcitations&#34;&gt;Boettiger, 2013&lt;/a&gt;&lt;/span&gt;).&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;lcolladotor, lcollado753. &lt;em&gt;GitHub&lt;/em&gt; &lt;a href=&#34;https://github.com/lcolladotor/lcollado753/tree/master/final/nfl_half&#34;&gt;&lt;a href=&#34;https://github.com/lcolladotor/lcollado753/tree/master/final/nfl_half&#34;&gt;https://github.com/lcolladotor/lcollado753/tree/master/final/nfl_half&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;denishaine, (2013) Veterinary Epidemiologic Research: GLM &amp;amp;ndash; Evaluating Logistic Regression Models (part 3). &lt;em&gt;denis haine&lt;/em&gt; &lt;a href=&#34;http://denishaine.wordpress.com/2013/03/19/veterinary-epidemiologic-research-glm-evaluating-logistic-regression-models-part-3/&#34;&gt;&lt;a href=&#34;http://denishaine.wordpress.com/2013/03/19/veterinary-epidemiologic-research-glm-evaluating-logistic-regression-models-part-3/&#34;&gt;http://denishaine.wordpress.com/2013/03/19/veterinary-epidemiologic-research-glm-evaluating-logistic-regression-models-part-3/&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Advanced NFL Stats. &lt;a href=&#34;http://www.advancednflstats.com/&#34;&gt;&lt;a href=&#34;http://www.advancednflstats.com/&#34;&gt;http://www.advancednflstats.com/&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;(2010) Advanced NFL Stats: Play-by-Play Data. &lt;a href=&#34;http://www.advancednflstats.com/2010/04/play-by-play-data.html&#34;&gt;&lt;a href=&#34;http://www.advancednflstats.com/2010/04/play-by-play-data.html&#34;&gt;http://www.advancednflstats.com/2010/04/play-by-play-data.html&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;(2009) Advanced NFL Stats: How the Model Works–A Detailed Example Part 1. &lt;a href=&#34;http://www.advancednflstats.com/2009/01/how-model-works-detailed-example.html&#34;&gt;&lt;a href=&#34;http://www.advancednflstats.com/2009/01/how-model-works-detailed-example.html&#34;&gt;http://www.advancednflstats.com/2009/01/how-model-works-detailed-example.html&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;(2009) Advanced NFL Stats: How the Model Works–A Detailed Example Part 2. &lt;a href=&#34;http://www.advancednflstats.com/2009/01/how-model-works-detailed-example-part-2.html&#34;&gt;&lt;a href=&#34;http://www.advancednflstats.com/2009/01/how-model-works-detailed-example-part-2.html&#34;&gt;http://www.advancednflstats.com/2009/01/how-model-works-detailed-example-part-2.html&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;By BURKE, Brian Burke - The Fifth Down Blog - NYTimes.com. &lt;em&gt;The Fifth Down Â» Brian Burke&lt;/em&gt; &lt;a href=&#34;http://fifthdown.blogs.nytimes.com/author/brian-burke/&#34;&gt;&lt;a href=&#34;http://fifthdown.blogs.nytimes.com/author/brian-burke/&#34;&gt;http://fifthdown.blogs.nytimes.com/author/brian-burke/&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Carl Boettiger, knitcitations: Citations for knitr markdown files. &lt;a href=&#34;https://github.com/cboettig/knitcitations&#34;&gt;&lt;a href=&#34;https://github.com/cboettig/knitcitations&#34;&gt;https://github.com/cboettig/knitcitations&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RStudio , Inc. , (2013) shiny: Web Application Framework for R. &lt;a href=&#34;http://CRAN.R-project.org/package=shiny&#34;&gt;&lt;a href=&#34;http://CRAN.R-project.org/package=shiny&#34;&gt;http://CRAN.R-project.org/package=shiny&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing SimplyStatistics visits info</title>
      <link>https://lcolladotor.github.io/2013/03/09/analyzing-simplystatistics-visits-info/</link>
      <pubDate>Sat, 09 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2013/03/09/analyzing-simplystatistics-visits-info/</guid>
      <description>&lt;p&gt;Recently we had to analyze the data of the number of visits per day to &lt;a href=&#34;http://simplystatistics.org/&#34;&gt;SimplyStatistics.org&lt;/a&gt;. There were two goals:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Estimate the fraction of visitors retained after a spike in the number of visitors&lt;/li&gt;
&lt;li&gt;Identify (if any) any factors that influence the fraction estimated in 1.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;For me it was a fun project in part because I like SimplyStatistics but also because I think that finding the answers to the questions would be interesting and help understand the readers of that blog.&lt;/p&gt;
&lt;p&gt;Sadly, I didn&amp;#8217;t work on it much. We had lots of stuff due that week, but well, I&amp;#8217;m happy enough with the analysis I did. My own report is hosted &lt;a href=&#34;https://github.com/lcolladotor/lcollado753/tree/master/hw/data-analysis-02&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/lcolladotor/lcollado753/blob/master/hw/data-analysis-02/report/data_02_lcollado.pdf&#34; target=&#34;_blank&#34;&gt;this is the pdf file of the report itself&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Half joking with other students, I said that I basically did t-tests. Hopefully I can work on changing this tendency with the pile of recommended books I&amp;#8217;ve been acquiring but not really reading through. Except for the &lt;a href=&#34;http://bit.ly/13MyHwt&#34;&gt;ggplot2: Elegant Graphics for Data Analysis&lt;/a&gt; and the &lt;a href=&#34;http://oreil.ly/Yk8xtl&#34;&gt;R Graphics Cookbook&lt;/a&gt;. Sounds like spring break will be fun :P&lt;/p&gt;
&lt;p&gt;Kind of related to this, &lt;a href=&#34;http://bit.ly/13MypWw&#34;&gt;Jeff Leek announced yesterday that he is going to  compile a list of student blogs that have something to do with statistics and data&lt;/a&gt;. He added a link to my blog which is why I saw a large peak of Fellgernon Bit&amp;#8217;s visitor data. After all, when doing the data analysis described above I played around with the data from Fellgernon Bit and now know that at a minimum posting drives visitor&amp;#8217;s into sites (which sounds obvious, but maybe you get random traffic) —see &lt;a href=&#34;https://github.com/lcolladotor/lcollado753/blob/master/hw/data-analysis-02/report/data_02_lcollado.pdf&#34;&gt;fig 1 of the report&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image&#34; src=&#34;http://media.tumblr.com/f5ce3511fb8d6899a613e348a846dcc8/tumblr_inline_mjf4iavs4A1qz4rgp.png&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Had Jeff done so before, I could have a point estimate (but without being able to say something about the uncertainty of it) that SimplyStatistics has 142 visitors that read the posts AND click on the links. Maybe using the info from &lt;a href=&#34;http://bit.ly/12vVmbp&#34;&gt;Hilary&amp;#8217;s&lt;/a&gt; and &lt;a href=&#34;http://bit.ly/13MyyZS&#34;&gt;Alyssa&amp;#8217;s&lt;/a&gt; blogs we could have an estimate with some measure of uncertainty, but only for March 8th.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Second cultural mixer today!</title>
      <link>https://lcolladotor.github.io/2013/02/15/second-cultural-mixer-today/</link>
      <pubDate>Fri, 15 Feb 2013 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2013/02/15/second-cultural-mixer-today/</guid>
      <description>&lt;img src=&#34;http://24.media.tumblr.com/f55efedc3ca5189bcc1600941f7ef56c/tumblr_mi58wrDjuB1qgn8kjo1_500.png&#34;&gt;
</description>
    </item>
    
    <item>
      <title>Sharing my work for &#34;Advanced Methods III&#34;</title>
      <link>https://lcolladotor.github.io/2013/02/13/sharing-my-work-for-advanced-methods-iii/</link>
      <pubDate>Wed, 13 Feb 2013 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2013/02/13/sharing-my-work-for-advanced-methods-iii/</guid>
      <description>&lt;p&gt;This semester I&amp;#8217;m taking the live version of the Data Analysis class by Jeff Leek. His more &lt;a href=&#34;https://class.coursera.org/dataanalysis-001/class/index&#34;&gt;popular version of the course is available through Coursera&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;One of the things that Jeff promotes is reproducibility and sharing code. I share that tendency and thus created a Git repository for my homework and code for the class: &lt;a href=&#34;http://bit.ly/12vSk7d&#34;&gt;lcollado753&lt;/a&gt;. I&amp;#8217;m hosting it with GitHub to try it out since I started with Mercurial via Bitbucket. &lt;/p&gt;
&lt;p&gt;Part of me would love it if everyone in the class had their own Git repositories. I mean, this class involves lots of practice exercises and there are plenty of R packages and functions that others use that I would like to learn. As I don&amp;#8217;t see this happening, I think that it would be great to list the packages/functions you think could be interesting to others at the end of the write-ups. However, this involves sharing the reports and I don&amp;#8217;t know if that will happen.&lt;/p&gt;
&lt;p&gt;But maybe I didn&amp;#8217;t get the instructions Jeff gave correctly the first time. Listening into his week 2 talks from the Coursera course, I get that he wants our reports to be reproducible. The idea is great, but sometimes I get lots in the technicalities of finding the best fit for our situation. Aka, something we can all do that is worth the time for small scale projects that we have a couple of days to complete and most likely will be finishing the day before they are due. For now we might stick to sharing zip files with the report + summarized data set (it has be small enough to be sharable by email).&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m pretty happy with hosting my stuff at GitHub. One blunder I made in the&lt;a href=&#34;https://github.com/lcolladotor/lcollado753/blob/master/hw/data-analysis-01/report/data01_lcollado.pdf&#34;&gt; first data analysis report&lt;/a&gt; is that I completely forgot to say in it that I have the code in GitHub :P Oh well, next time!&lt;/p&gt;
&lt;p&gt;I feel that I also have lots to improve regarding how to tell a story in a report. Plus, for this first project I mainly did some exploratory data analysis without much stat analysis.&lt;/p&gt;
&lt;p&gt;Overall, I&amp;#8217;m quite excited with this course =) and I think that I&amp;#8217;ll learn a ton on methods to analyze data AND how to actually implement them. Plus, I&amp;#8217;m currently trying to learn ggplot2 as you can see in that first report. Also, I made it with knitr instead of Sweave =)&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why aren&#39;t all of our graphs interactive?</title>
      <link>https://lcolladotor.github.io/2012/10/08/why-arent-all-of-our-graphs-interactive/</link>
      <pubDate>Mon, 08 Oct 2012 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2012/10/08/why-arent-all-of-our-graphs-interactive/</guid>
      <description>&lt;p&gt;During the last pre-happy hour seminar, &lt;a href=&#34;http://www.biostat.wisc.edu/~kbroman/&#34;&gt;Karl Broman&lt;/a&gt; talked about &lt;a href=&#34;http://www.biostat.wisc.edu/~kbroman/presentations/DynamicGraphs/&#34;&gt;Why aren&amp;#8217;t all of our graphs interactive?&lt;/a&gt; I didn&amp;#8217;t know, but a few years ago Karl worked in the department and clearly promoted beer-drinking and is &lt;em&gt;the heart of the department. &lt;/em&gt;I&amp;#8217;m a fan of our pre-happy hour seminars since you have a get to listen to good/fun talks over a beer or two.&lt;/p&gt;
&lt;p&gt;But I&amp;#8217;m also a fan of reproducible research and useful graphics. I do most of this by using &lt;a href=&#34;http://www.statistik.lmu.de/~leisch/Sweave/&#34;&gt;Sweave&lt;/a&gt; (for reproducibility) in LaTeX documents and with the R packages &lt;a href=&#34;http://cran.r-project.org/web/packages/lattice/index.html&#34;&gt;lattice&lt;/a&gt;, &lt;a href=&#34;http://cran.r-project.org/web/packages/car/index.html&#34;&gt;car&lt;/a&gt;, and &lt;a href=&#34;http://cran.r-project.org/web/packages/plotrix/index.html&#34;&gt;plotrix&lt;/a&gt;, and some &lt;a href=&#34;http://ggplot2.org/&#34;&gt;ggplot2&lt;/a&gt; (I should use it more). &lt;/p&gt;
&lt;p&gt;Karl made &lt;a href=&#34;http://www.biostat.wisc.edu/~kbroman/presentations/DynamicGraphs/&#34;&gt;his presentation&lt;/a&gt; using html (definitely check it out!) and inserted pretty interactive graphics. His talk got me really interested and I definitely need to pick up a few tools. For example, asciidoc or R Markdown can be useful for making html documents with R code. Specially if you want to write a report and you don&amp;#8217;t want to deal with Sweave/Latex when making plots (can be a pain to know where they&amp;#8217;ll show up). &lt;/p&gt;
&lt;p&gt;For the interactive side, D3 (and other tools Karl listed) can be useful to learn. But I might put this on a hold for some time. Maybe I&amp;#8217;ll wait and see what others in the deparment are developing for R-D3 and embedding interactive plots in pdf files.&lt;/p&gt;
&lt;p&gt;I don&amp;#8217;t think that it will be long before interactive plots make it to the journals. Specially for their web versions. Though, I still think that if you are showing a 3D plot, as the author you will have to give a few default views where you can clearly see something that you want to talk about instead of having the reader find that sweet spot. &lt;/p&gt;
&lt;p&gt;One problem that I don&amp;#8217;t think has been solved yet is reproducible research on a cluster. Karl and others mentioned &lt;a href=&#34;http://www.gnu.org/software/make/manual/make.html&#34;&gt;make&lt;/a&gt; as well as having if/else clauses where you either show the output or a cleaned up version of the code that you used to generate the output. &lt;/p&gt;
&lt;p&gt;Overall, there are many tools and tips I can learn from Karl. And I&amp;#8217;m sure that I&amp;#8217;m not the only one! Hopefully he&amp;#8217;ll give tips on where to start (nothing is more tedious than reading UNIX man-files).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JHSPH-Biostat through Coursera</title>
      <link>https://lcolladotor.github.io/2012/10/01/jhsph-biostat-through-coursera/</link>
      <pubDate>Mon, 01 Oct 2012 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2012/10/01/jhsph-biostat-through-coursera/</guid>
      <description>&lt;p&gt;Have you heard of online education? If you are in the US or Mexico I&amp;#8217;m sure that you have seen some ads about online universities. Well, that&amp;#8217;s not the type of education I&amp;#8217;m talking about. I&amp;#8217;m talking about free high-quality education. &lt;/p&gt;
&lt;p&gt;For some years, the top option has been the &lt;strong&gt;O&lt;/strong&gt;pen &lt;strong&gt;C&lt;/strong&gt;ourse&lt;strong&gt;w&lt;/strong&gt;are (OCW) organized under the &lt;a href=&#34;http://www.ocwconsortium.org/&#34;&gt;Open Courseware Consortium&lt;/a&gt; (OCWC). Back in 2009 I was pushed my undergrad (LCG-UNAM) to design and teach OCW-compliant courses. I even taught a &lt;a href=&#34;http://www.lcg.unam.mx/~lcollado/B/index_en.html&#34;&gt;course on R/Bioconductor&lt;/a&gt; and thought of it as a pilot OCW course. The first seven classes were video recorded. But that project hit a wall because many of the biology professors used slides that heavily relied on copyrighted material. For OCW courses you have to own the copyright of the material that you use (or get permission), so just the idea of having to re-do all the diagrams and figures was overwhelming. This hasn&amp;#8217;t stopped some big universities like &lt;a href=&#34;http://ocw.mit.edu/index.htm&#34;&gt;MIT from publishing OCW-compliant courses&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Recently there&amp;#8217;s been talk of the new horse in the race: &lt;a href=&#34;https://www.coursera.org/&#34;&gt;Coursera&lt;/a&gt;. What is it? Well, &lt;a href=&#34;https://www.coursera.org/about&#34;&gt;according to themselves&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We are a social entrepreneurship company that partners with the top universities in the world to offer courses online for anyone to take, for free. We envision a future where the top universities are educating not only thousands of students, but millions. Our technology enables the best professors to teach tens or hundreds of thousands of students.&lt;/p&gt;
&lt;p&gt;Through this, we hope to give everyone access to the world-class education that has so far been available only to a select few. We want to empower people with education that will improve their lives, the lives of their families, and the communities they live in.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So far, the motivation is similar to the OCW movement. However, one very big difference is that Coursera &lt;strong&gt;does offer certificates&lt;/strong&gt;. Something which OCW courses do not. For example, &lt;a href=&#34;http://ocw.mit.edu/about/&#34;&gt;MIT-OCW&lt;/a&gt; says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;OCW is not an MIT education.&lt;/li&gt;
&lt;li&gt;OCW does not grant degrees or certificates.&lt;/li&gt;
&lt;li&gt;OCW does not provide access to MIT faculty.&lt;/li&gt;
&lt;li&gt;Materials may not reflect entire content of the course.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Coursera courses do provide the entire content of the course. Well, this is slightly tricky since some professors use the same base material in the university-in-class courses but expand it beyond what is available through Coursera. Thus in a sense Coursera are more accesible courses with lesser requirements than the in-class versions. But compared to OCW, you have homeworks (which are graded) and can communicate with the faculty through the use of forums.&lt;/p&gt;
&lt;p&gt;One advantage of OCW courses is that you can look at them whenever you want. For Coursera ones you have to sign up (and thus register to their system) and they are open for certain periods of time.&lt;/p&gt;
&lt;p&gt;Currently, the Biostatistics Department at JHSPH is offering three courses through Coursera. These are &lt;a href=&#34;https://www.coursera.org/course/compdata&#34;&gt;Computing for Data Analysis&lt;/a&gt; by &lt;a href=&#34;http://www.biostat.jhsph.edu/~rpeng/&#34;&gt;Roger D. Peng&lt;/a&gt;, &lt;a href=&#34;https://www.coursera.org/course/biostats&#34;&gt;Mathematical Biostatistics Boot Camp&lt;/a&gt; by &lt;a href=&#34;http://www.bcaffo.com/&#34;&gt;Brian Caffo&lt;/a&gt;, and &lt;a href=&#34;https://www.coursera.org/course/dataanalysis&#34;&gt;Data Analysis&lt;/a&gt; by &lt;a href=&#34;http://www.biostat.jhsph.edu/~jleek/&#34;&gt;Jeffrey Leek&lt;/a&gt;. The first two are introductory courses to using R and Biostatistics, respectively. I&amp;#8217;m taking the in-class versions and highly recommend them to anyone that wants to get started in either topic. They both involve youtube videos and practice exercises. The videos themselves are great since they rehearse what they are going to say, used a high-quality audio recording room, tuned the audio, and included highlights in the slides so you can follow them easily. &lt;strong&gt;Right now you can go and sign up for these two courses!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The third one, Data Analysis, is more advanced and I&amp;#8217;ll take it in-class next year. In addition, for now the sign up is closed for 2012 (you can go ahead and save a spot for 2013). &lt;/p&gt;
&lt;p&gt;All of these courses have a couple thousands students registered, which is great! I&amp;#8217;m sure that the great majority will greatly benefit from them. To finish my post, I&amp;#8217;ll leave you with their short introduction videos, which will tell you more than what I can via text!&lt;/p&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
&lt;p&gt;&lt;iframe frameborder=&#34;0&#34; height=&#34;253&#34; src=&#34;http://www.youtube.com/embed/gk6E57H6mTs&#34; width=&#34;450&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;&lt;iframe frameborder=&#34;0&#34; height=&#34;253&#34; src=&#34;http://www.youtube.com/embed/ekdpaf_WT_8&#34; width=&#34;450&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;&lt;iframe frameborder=&#34;0&#34; height=&#34;253&#34; src=&#34;http://www.youtube.com/embed/-lutj1vrPwQ&#34; width=&#34;450&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quid @Biostat-JHSPH</title>
      <link>https://lcolladotor.github.io/2012/09/28/quid-biostat-jhsph/</link>
      <pubDate>Fri, 28 Sep 2012 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2012/09/28/quid-biostat-jhsph/</guid>
      <description>&lt;p&gt;Just like most scientific departments, we have a seminar (weekly over here) where very bright people come to us to talk about their work. Being a Biostatistics department, we mostly get faculty from other Biostatistics departments from universities to talk to us. This week was quite different. Amy Heineike from &lt;strong&gt;&lt;a href=&#34;http://quid.com/&#34;&gt;Quid&lt;/a&gt;&lt;/strong&gt; gave us a talk describing their product, which fits perfectly in what is now called &amp;#8220;data science&amp;#8221;. You can see Amy at the end of the table in the picture below.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Amy is the one at the end of the table.&#34; height=&#34;168&#34; src=&#34;http://quid.com/img/life/team.jpg&#34; width=&#34;299&#34;/&gt;&lt;/p&gt;
&lt;p&gt;So what is Quid? It&amp;#8217;s a start up tech company that provides either their software or reports derived from it that help big companies (a) analyze a field, (b) look at what the competition is doing, (c) take informed decisions (helpful for marketing). The short video below describes Quid in a more general way, check it out!&lt;/p&gt;
&lt;p&gt;&lt;iframe frameborder=&#34;0&#34; height=&#34;253&#34; src=&#34;http://www.youtube.com/embed/5hGTjhuimH0&#34; width=&#34;450&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;As Amy Heineike described in her talk, the three common decision-taking pathways are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Someone follows their own intuition. Say a big shot that thinks he knows where the world is going.&lt;/li&gt;
&lt;li&gt;Someone with decision power asks others to generate reports for her/him. That is, lots of manual work where some read, consult others, etc then they summarize the information in a report.&lt;/li&gt;
&lt;li&gt;Similar to the above one where lots of people gather the information, then a program is run and the decision is pretty much made by the computer.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The Quid paradigm is to use the computer to gather all the information and then have a human(s) look at a network with a very cool 3D tool to assimilate the information and decide themselves. The argument is that the human brain is very powerful for visual pattern recognition and can out-perform computers. &lt;/p&gt;
&lt;p&gt;At first I felt that you can do the network part with a software like &lt;a href=&#34;http://www.cytoscape.org/&#34;&gt;Cytoscape&lt;/a&gt; which I find to be very powerful for network analysis. But the pipeline used by Quid is much more extensive and it&amp;#8217;s an all-in-one bundle.&lt;/p&gt;
&lt;p&gt;Another key argument in favor of Quid is that most of the information shared is done in a list format. Like google search results, powerpoint bullet points, your facebook feed, etc. But who came up with the ranking? How are things related? That&amp;#8217;s when you need a network representation.&lt;/p&gt;
&lt;p&gt;I recommend taking a look at their &lt;a href=&#34;http://quid.com/technical.php&#34;&gt;technical overview page&lt;/a&gt; where they have the main steps outlined. But needless to say, they depend strongly on the natural language processing early steps. Their 3D tool looked very interesting and I love to play with it. Amy Heineike actually poked us by showing a video of a short session using the software that was designed so we would want to have a go with Quid. I, as many others, were hooked! Sadly, Quid&amp;#8217;s software is not the kind that academics can go buy for now.&lt;/p&gt;
&lt;p&gt;I found the example using &amp;#8220;synthetic biology&amp;#8221; as the query to be pretty interesting. Sadly I don&amp;#8217;t have a picture, but one of the features that seems very powerful is when you change to a 2D display. In it, you have the time on the X-axis and the number of articles (well, any kind of input file Quid can use) on the Y-axis. By clicking on a point (which corresponds to a node in the network 3D environment) you can then visualize all the connections that are directly linked to it. Thus you have a scatterplot with a 2D network on top of it. That information can be really useful to understand the flow of information. The specific example was how someone proposed years ago that a specific kind of application was possible, time later grants on the subject were announced, and more close to the present he got a grant, then other grants and results were publicized.&lt;/p&gt;
&lt;p&gt;Now, Quid has some flaws. For instance, one hot question was how to control the threshold that determines whether two nodes are connected or not. The answer was something like this: experts in their fields have validated the results for queries related to them. Not very convincing for a biostat crowd. Another one was how to control/remove/correct bias. Amy Heineike replied that you need to learn where the data used by Quid is like. For example, when looking at companies the number of news articles mentioned is linked to how efficient/big their public relations office is.&lt;/p&gt;
&lt;p&gt;Nevertheless, Quid&amp;#8217;s product is very interesting. Plus, I feel that part of our tool-box as Biostatisticians is visualizing data in ways that allow us to understand what is going on. As for working at Quid or doing anything alone the line, we definitely need to learn more about computer science. After all, you need incredibly fast algorithms and code to work with enormous data sets. &lt;/p&gt;
&lt;p&gt;&amp;#8212;&amp;#8212;&lt;/p&gt;
&lt;p&gt;PS Amy Heineike might develop a &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/&#34;&gt;Pubmed&lt;/a&gt; scrapper for Quid. Meaning that Quid would be able to access citations data. Then it would be very cool to use a few &amp;#8220;seed&amp;#8221; papers that you are interested in to find the complete history behind them and any other papers similar to them. There might another group out there working in your field that you don&amp;#8217;t know about! Which I think happens more frequently that what you think. Specially if you don&amp;#8217;t look abroad.&lt;/p&gt;
&lt;p&gt;&amp;#8212;&amp;#8212;&lt;/p&gt;
&lt;p&gt;Edit: I had completely forgotten that I had read about &lt;a href=&#34;http://simplystatistics.org/post/19572022804/interview-with-amy-heineike-director-of-mathematics&#34;&gt;Amy Heineike before in her SimpleStatistics interview&lt;/a&gt;. There&amp;#8217;s more about her in this video and in &lt;a href=&#34;http://thephenomlist.com/lists/8/people/32&#34;&gt;The Phenomlist&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;iframe frameborder=&#34;0&#34; height=&#34;253&#34; src=&#34;http://www.youtube.com/embed/IVdwJvQXeg4&#34; width=&#34;450&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing Git while making your academic webpage</title>
      <link>https://lcolladotor.github.io/2012/09/24/introducing-git-while-making-your-academic-webpage/</link>
      <pubDate>Mon, 24 Sep 2012 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2012/09/24/introducing-git-while-making-your-academic-webpage/</guid>
      <description>&lt;p&gt;Last week I gave a presentation during our computing club on how to use git (a version control system). I used as a motivating example the first steps of creating your own academic webpage. The goal was to make it interesting to both new students (who might have been more interested on the webpage part) and older students (for whom version control should be a must). The slides and all the material is publicly available through the following Bitbucket repository: &lt;a href=&#34;https://bitbucket.org/lcolladotor/html_git_intro/overview&#34;&gt;&lt;a href=&#34;https://bitbucket.org/lcolladotor/html_git_intro/overview&#34;&gt;https://bitbucket.org/lcolladotor/html_git_intro/overview&lt;/a&gt;&lt;/a&gt;. You can access the slides by clicking on &amp;#8220;Source&amp;#8221;, &amp;#8220;slides&amp;#8221; and then &amp;#8220;html_git.pdf&amp;#8221;.&lt;/p&gt;
&lt;p&gt;For the talk, I tried to make it more interactive but at the same time I wanted to make sure that the material could work for reference in the future. For example, I added a &lt;em&gt;commands.txt&lt;/em&gt; file so anyone following me could easily copy-paste the commands. By the way, for Git Bash in Windows, you paste stuff by using the insert key instead of the usual &amp;#8220;ctrl + v&amp;#8221; shortcut.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m posting about it as it could be useful to other people, so feel free to share it.&lt;/p&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setting up your computer for bioinformatics/biostatistics and a compedium of resources</title>
      <link>https://lcolladotor.github.io/2012/08/23/setting-up-your-computer-for/</link>
      <pubDate>Thu, 23 Aug 2012 00:00:00 +0000</pubDate>
      <guid>https://lcolladotor.github.io/2012/08/23/setting-up-your-computer-for/</guid>
      <description>&lt;p&gt;Jumping on the train set by &lt;a href=&#34;http://www.biostat.jhsph.edu/~hiparker/&#34;&gt;Hilary Parker&lt;/a&gt; &amp;#8220;&lt;a href=&#34;http://hilaryparker.com/2012/08/16/the-setup-part-1/&#34;&gt;The Setup (Part 1)&lt;/a&gt;&#34; and &lt;a href=&#34;http://biostat.jhsph.edu/~afrazee/&#34;&gt;Alyssa Frazee&lt;/a&gt; &amp;#8220;&lt;a href=&#34;http://alyssafrazee.wordpress.com/&#34;&gt;my software/hardware setup&lt;/a&gt;&amp;#8221;, I&amp;#8217;m going to share my setup and hopefully add something new. They both did a great job already, so make sure you read their posts!&lt;/p&gt;
&lt;p&gt;I have some experience with all three main OS: Windows, Linux and Mac. That being said, I know some of the basic stuff for each but I surely use Google very frequently to get help. I used to have a dual Windows / Linux (Ubuntu) set up but now I have a Windows laptop/desktop (it&amp;#8217;s a monster :P) at home and I&amp;#8217;m happy working with my Mac. &lt;/p&gt;
&lt;p&gt;I&amp;#8217;m going to start by mentioning the software I use(d) in each OS and then add some other tools that I really like.&lt;/p&gt;
&lt;p&gt;Windows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Text editor: &lt;a href=&#34;http://notepad-plus-plus.org/&#34;&gt;Notepad++&lt;/a&gt;. It outperforms Notepad by light years! A must for me is the “View -&amp;gt; Word wrap” option. I would definitely go to “Settings -&amp;gt; Preferences -&amp;gt; New Document/Default Directory” and change the new document format from Windows (Dos) to Unix. This will save you time later when you want to work on a Unix system like the cluster. If you didn’t, you can change a specific document’s EOL (end of line) by using “Edit -&amp;gt; EOL conversion -&amp;gt; UNIX format”. Another feature that I like is the “Search -&amp;gt; Replace…” which allows you to use regular expressions (like Perl). &lt;/li&gt;
&lt;li&gt;Statistical software: &lt;a href=&#34;http://cran.r-project.org/&#34;&gt;R&lt;/a&gt; of course! It’s best to do a custom installation and choose a directory without spaces in it. That will help later (further below). If you want to be convinced to join the R community read &lt;a href=&#34;http://www.nytimes.com/2009/01/07/technology/business-computing/07program.html?_r=3&amp;amp;scp=1&amp;amp;sq=robert%20gentleman&amp;amp;st=cse&#34;&gt;Data Analysits Captivated by R’s power&lt;/a&gt; and &lt;a href=&#34;http://bits.blogs.nytimes.com/2009/01/08/r-you-ready-for-r/?scp=1&amp;amp;sq=Robert%20Gentleman&amp;amp;st=cse&#34;&gt;R You Ready for R?&lt;/a&gt; or just take a look at what others are using. Remember that the R project is open source, free and easy to contribute to. If you end up choosing Excel as your statistical software, well, there is &lt;strong&gt;no&lt;/strong&gt; hope for you!!&lt;/li&gt;
&lt;li&gt;R code editor: Notepad++ with &lt;a href=&#34;http://sourceforge.net/projects/npptor/&#34;&gt;NppToR&lt;/a&gt;. For a long time I used Emacs modified to work with Windows by Vincent Goulet available &lt;a href=&#34;http://vgoulet.act.ulaval.ca/en/emacs/&#34;&gt;here&lt;/a&gt;. It works great and saves you quite a bit of setup time. &lt;a href=&#34;http://www.xemacs.org/&#34;&gt;XEmacs&lt;/a&gt; is another option that a friend of mine used, but it never convinced me. Anyhow, I ended up changing from Emacs to Notepad++ with NppToR because I could:&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Force quit R (and not lose code changes) in case I crashed R by doing something stupid like printing something huge or w/e :P &lt;/li&gt;
&lt;li&gt;Access help pages in a separate window. I’m sure you can do it too with Emacs, but I was just lazy to configure it.&lt;/li&gt;
&lt;li&gt;Shorter shortcuts&lt;/li&gt;
&lt;li&gt;Later on I found the NppToR to PuTTy feature which is very useful.&lt;/li&gt;
&lt;li&gt;You can create an R syntax dictionary (or something like that) in Notepad++ which will scan all your R packages and add the function names so they are colored when you type them. Also Notepad++ will auto-complete some function names and show you the arguments. Great stuff! (Forgot the name, so… google it :P)&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;SSH: &lt;a href=&#34;http://www.chiark.greenend.org.uk/~sgtatham/putty/&#34;&gt;PuTTY&lt;/a&gt;. As said before, works well with Notepad++ and NppToR.&lt;/li&gt;
&lt;li&gt;SCP: &lt;a href=&#34;http://winscp.net/eng/index.php&#34;&gt;WinSCP&lt;/a&gt;. There are others that work too like &lt;a href=&#34;http://filezilla-project.org/&#34;&gt;Filezilla&lt;/a&gt; but well, WinSCP does the job well.&lt;/li&gt;
&lt;li&gt;PDF viewer: &lt;a href=&#34;http://www.adobe.com/products/acrobatpro.html&#34;&gt;Adobe Acrobat Professional&lt;/a&gt;. I’m using the X version now. I like how I can highlight, underline, cross out, free hand, sticky note, combine files into a single pdf, combine pdfs, and change the highlight colors easily. It also has a change tracker (kind of like Word has). I’ve seen other use PDF Annotator which is available for free for Hopkins students. Anyhow, I simply love Acrobat for reading papers.&lt;/li&gt;
&lt;li&gt;LaTeX: &lt;a href=&#34;http://miktex.org/&#34;&gt;MiKTeX&lt;/a&gt;. For writing TeX files I used either Emacs or Notepad++. There is another software which has drop down menus and the like called &lt;a href=&#34;http://www.winedt.com/&#34;&gt;WinEdt&lt;/a&gt;. I got used to typing LaTeX from scratch, well, I have a template.Rnw somewhere. Oh yeah, I always use Sweave when writing TeX files (even if I don’t use R). &lt;/li&gt;
&lt;li&gt;R reports: &lt;a href=&#34;http://www.statistik.lmu.de/~leisch/Sweave/&#34;&gt;Sweave by Friedrich Leisch&lt;/a&gt;, one of the champions of reproducibility! To learn more about Sweave first &lt;a href=&#34;http://stat.epfl.ch/webdav/site/stat/shared/Regression/EPFL-Sweave-powerdot.pdf&#34;&gt;read this pdf&lt;/a&gt; by Nicola Sartori. This is another &lt;a href=&#34;http://users.stat.umn.edu/~geyer/Sweave/foo.pdf&#34;&gt;Sweave demo&lt;/a&gt; by Charles J Geyer. Check out this great &lt;a href=&#34;http://www.johndcook.com/troubleshooting_sweave.html&#34;&gt;Windows Sweave troubleshooting page&lt;/a&gt; by John D Cook.&lt;/li&gt;
&lt;li&gt;Building R packages from source. You will definitely need &lt;a href=&#34;http://cran.fhcrc.org/bin/windows/Rtools/&#34;&gt;Rtools&lt;/a&gt; installed. I would also install &lt;a href=&#34;http://qpdf.sourceforge.net/&#34;&gt;QPDF&lt;/a&gt; which can be used by R to compress your pdf files, which is a good thing if you want to have a small-sized tarball. Last but not least, check out &lt;a href=&#34;http://robjhyndman.com/researchtips/building-r-packages-for-windows/&#34;&gt;Building R packages for Windows&lt;/a&gt; by Rob J Hyndman.&lt;/li&gt;
&lt;li&gt;Learn to modify your PATH! Check 1.3 from the previous link by Rob J Hyndman. If you are going to use Sweave, it’s best to add to your PATH the path for the directory containing your Sweave.sty file so that you won’t need to copy it to every single directory. This is why it pays off to do an R custom installation and put it in C:/R/R-current-version or something like that instead of C:/Program Files/ bla bla with spaces. It used to be more important a few years ago. Also, I created a sw.bat file and put it somewhere where my PATH would find it. That sw.bat file ran Sweave, pdflatex twice, then bibtex and finally opened the pdf file.&lt;/li&gt;
&lt;li&gt;PDF viewer for LaTeX files. I only learnt about pdf sync and the like a year ago. You should google how to set this up with SumatraPDF (Adobe Acrobat doesn’t work!).&lt;/li&gt;
&lt;li&gt;Version control: &lt;a href=&#34;http://mercurial.selenic.com/&#34;&gt;Mercurial&lt;/a&gt;. It’s very easy to use and you can get an account at &lt;a href=&#34;https://bitbucket.org/&#34;&gt;Bitbucket.org&lt;/a&gt; with unlimited number of private repositories if you have an academic email. Even if you are not doing a collaborative project, you will love using a version control system! It will clean up your directories very nicely and will help you become more organized. Learning a few commands is nothing compared to having lots of files with _v1 v_2, etc at the end. Check out the &lt;a href=&#34;http://mercurial.selenic.com/guide/&#34;&gt;Mercurial guide&lt;/a&gt; to get started. Note that for windows instead of customizing your .hgrc file you will customize a mercurial.ini file.&lt;/li&gt;
&lt;li&gt;Presentations: both PowerPoint and &lt;a href=&#34;http://en.wikipedia.org/wiki/Beamer_(LaTeX)&#34;&gt;Beamer&lt;/a&gt; (normally with Sweave too). Rarely I use &lt;a href=&#34;https://docs.google.com&#34;&gt;Google Docs&lt;/a&gt; for this.&lt;/li&gt;
&lt;li&gt;Office: Either Microsoft Office or &lt;a href=&#34;http://www.openoffice.org/&#34;&gt;OpenOffice&lt;/a&gt; (free).&lt;/li&gt;
&lt;li&gt;Poster creator: &lt;a href=&#34;http://www.postergenius.com/cms/index.php&#34;&gt;PosterGenius&lt;/a&gt; (academic discount price). It was very easy to use and I would surely give the free trial version a go. It adds a watermark, but well, you will appreciate the time you save compared to using PowerPoint. I guess that &lt;a href=&#34;http://www.adobe.com/products/photoshop.html&#34;&gt;Adobe Photoshop&lt;/a&gt; is another option, but I’ve only used it to edit photos here and there, not to make a whole poster. The most I did was create &lt;a href=&#34;https://picasaweb.google.com/lh/photo/NUiUno0H6MZ8dfTN8ZqF9yRO-Af4bmaijyeT4dbBml0?feat=embedwebsite&#34;&gt;this&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;To de-compress RAR files: &lt;a href=&#34;http://www.win-rar.com/start.html?&amp;amp;L=0&#34;&gt;WinRAR&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Anti-spyware: &lt;a href=&#34;http://www.avast.com/free-antivirus-download&#34;&gt;Avast&lt;/a&gt; (free version). I normally keep it in silent (gaming) mode so it doesn’t show pop ups.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cccp-project.net/&#34;&gt;CCCP codec pack&lt;/a&gt; which includes the Media Player Classic. Great for watching video files and dumping the crappy Windows Media Player.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Linux (Ubuntu)&lt;/p&gt;
&lt;p&gt;Ubuntu provides Linux distributions that are very user friendly and that look much like Mac OS does now. You&amp;#8217;ll find it easy to run multi-core programs which were a pain to do with Windows. Beware that even if you use Ubuntu you will need to learn stuff like how to compile. Also, please check before you install that your computer is supported. For example, some laptops with very new video cards might not work properly. That being said, with Ubuntu you will feel very at ease working in an area like mine (genomics) because a lot of the software runs in Linux (normally in a cluster, but you can test in your lap).&lt;/p&gt;
&lt;p&gt;You will want to check and/or keep for reference &lt;a href=&#34;http://faculty.ucr.edu/~tgirke/Documents/UNIX/linux_manual.html&#34;&gt;LINUX Essentials&lt;/a&gt; by Thomas Girke (more from him below), &lt;a href=&#34;http://freeengineer.org/learnUNIXin10minutes.html&#34;&gt;Learn Linux in 10 minutes&lt;/a&gt;, &lt;a href=&#34;http://www.linux-tutorial.info/toc&#34;&gt;The Linux tutorial&lt;/a&gt;, and &lt;a href=&#34;http://www.basicconfig.com/linux/vi&#34;&gt;Linux vi editor tutorial&lt;/a&gt;.  &lt;/p&gt;
* If you are going to use Linux (Ubuntu) at some point you will want to compile something from source and find out that you are missing a dependency. That&amp;#8217;s when I google, then use&amp;#160;:
1. apt-cache search something
2. sudo apt-get install something 
&lt;ul&gt;
&lt;li&gt;Note that you will frequently need the yyyy-devel version which includes c headers and stuff that you need to compile.&lt;/li&gt;
&lt;li&gt;You will find a lot of things through the package installer (forgot what it’s called). Learn the pseudonym for your Ubuntu version so you select the appropriate version of the software in case that you are downloading it from another place.&lt;/li&gt;
&lt;li&gt;SSH/SCP: terminal commands :) I just wanted to mention that rsync is a nice command for synching folders (recursively too) between your computer and say the cluster.&lt;/li&gt;
&lt;li&gt;Version control: Mercurial again. The configuration file is .hgrc not mercurial.ini&lt;/li&gt;
&lt;li&gt;Text editor: Nedit or Emacs. Vi when doing in-terminal modifications.&lt;/li&gt;
&lt;li&gt;You can get R through aptitude or if you want the very latest (or a devel version) you’ll have to compile it. The first time you will have to install plenty of dependencies, but it’s good practice.&lt;/li&gt;
&lt;li&gt;Office: go with OpenOffice.&lt;/li&gt;
&lt;li&gt;LaTeX: install the texlive distribution. I normally get everything so that later when I’m trying to use a TeX package I won’t have to go install it (which is what MiKTeX does for you in Windows). &lt;/li&gt;
&lt;li&gt;Video player: VLC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mac&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Terminal: &lt;a href=&#34;http://www.iterm2.com&#34;&gt;iTerm2&lt;/a&gt;. Mac comes with a native terminal, but iTerm2 has other nice functions like tabs and more options to customize it. Check &lt;a href=&#34;http://code.google.com/p/iterm2/wiki/ColorGallery&#34;&gt;this&lt;/a&gt; for free color palettes. I like the Homebrew one from &lt;a href=&#34;https://github.com/mbadolato/iTerm2-Color-Schemes&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;LaTeX and R editor: &lt;a href=&#34;http://aquamacs.org/&#34;&gt;Aquamacs&lt;/a&gt; is a version of Emacs that works great. However, as I discussed in the Windows section I’m moving away from Emacs. Well, to be honest, I don’t want to put the time to learn how to customize Emacs properly and do amazing stuff with it like Kasper does. Recently (since June) I’ve been using &lt;a href=&#34;http://macromates.com/&#34;&gt;TextMate&lt;/a&gt;. It has this thing called “Bundles” which provides different hotkeys depending on the file you are editing. Meaning that for Rnw files you can Sweave them directly there and for R files you can either send the code to R or to the terminal (much like Notepad++). The one thing is that it is not free BUT there is a 2.0 alpha release &lt;a href=&#34;https://github.com/textmate/textmate&#34;&gt;available on github&lt;/a&gt; that you can compile. &lt;a href=&#34;http://developers.slashdot.org/story/12/08/09/1947234/textmate-2-released-as-open-source&#34;&gt;This lengthly discussion&lt;/a&gt; can be worth reading if you want to know more about the 2.0 version and the future of TextMate. Someone said there that &lt;a href=&#34;http://www.sublimetext.com/&#34;&gt;Sublime&lt;/a&gt; might be replacing TextMate but I haven’t looked for any R integration in it. Anyhow, I liked how TextMate included an auto-spell checker that recognizes Sweave/LaTeX code from the box :)&lt;/li&gt;
&lt;li&gt;Package installer: &lt;a href=&#34;http://www.macports.org/&#34;&gt;MacPorts&lt;/a&gt;. It’s kind of similar to aptitude from Linux but it’s Mac only. Note that you will definitely need to get &lt;a href=&#34;https://developer.apple.com/xcode/&#34;&gt;XCode&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;PDF viewer: also Adobe Acrobat Pro for the reasons mentioned previously.&lt;/li&gt;
&lt;li&gt;PDF viewer for LaTeX: TeXShop which I think comes with the MacTeX distribution. It has the forward sync that Alyssa mentions in her blog.&lt;/li&gt;
&lt;li&gt;Text editor: &lt;a href=&#34;http://www.barebones.com/products/TextWrangler/&#34;&gt;TextWrangler&lt;/a&gt;. Has several of the functions I talked about in the Notepad++ section like search and replace with regular expressions. It definitely outperforms the native text editor.&lt;/li&gt;
&lt;li&gt;SCP: &lt;a href=&#34;http://cyberduck.ch&#34;&gt;Cyberduck&lt;/a&gt;. I haven’t tried others, but it works and I’m happy with it. I also use the terminal to push/retrieve files like I would do in Linux. Same for ssh and Mercurial.&lt;/li&gt;
&lt;li&gt;Version control: Mercurial. Note that you might have to add a site key (like bitbucket’s) to your .hgrc file so it doesn’t complain when pushing files.&lt;/li&gt;
&lt;li&gt;Productivity: &lt;a href=&#34;http://itunes.apple.com/us/app/my-little-pomodoro/id412699095?mt=12&#34;&gt;My Little Pomodoro&lt;/a&gt; available from the Mac app store. I love it for following the Pomodoro technique (you can use any other timer that you like) which Hilary introduced me to. It works like a charm when you are under stress and need to be productive. After all, I’m prone to escape the stress and distract myself, so this helps me keep my distractions limited. I’ve also found that when I’m stuck in a problem and I take the 5 min break thinking about something else, well, the machine keeps working and when I come back from the break I have a new idea to try out.&lt;/li&gt;
&lt;li&gt;Video player: &lt;a href=&#34;http://www.videolan.org/vlc/index.html&#34;&gt;VLC&lt;/a&gt; (includes codecs).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other stuff&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Browser: I used to love&lt;a href=&#34;http://www.mozilla.org/en-US/&#34;&gt;Mozilla Firefox&lt;/a&gt; (it has a nice sync functionality) but I’ve moved to &lt;a href=&#34;https://www.google.com/intl/en/chrome/browser/&#34;&gt;Google Chrome&lt;/a&gt;. It’s kind of a shame that Google started to compete with Mozilla, but oh well bye bye 2007. I use Chrome because it works a tad bit better with other Google tools, but that’s it. It also syncs your bookmarks. Both work great and &lt;a href=&#34;http://www.opera.com/&#34;&gt;Opera&lt;/a&gt; is still my favorite backup browser. I guess anything but Internet Explorer and Safari.&lt;/li&gt;
&lt;li&gt;Learning R. I would definitely check Thomas Girke &lt;a href=&#34;http://manuals.bioinformatics.ucr.edu/home/programming-in-r&#34;&gt;Programming in R&lt;/a&gt; page and Frank McCown’s &lt;a href=&#34;http://www.harding.edu/fmccown/R/&#34;&gt;Producing Simple Graphs with R&lt;/a&gt; tutorial. I have my own share of R related slides &lt;a href=&#34;http://www.biostat.jhsph.edu/~lcollado/teaching.html#.UDblsWie5Ng&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Learning Bioconductor. Thomas Girke again wrote a great resource for learning how to use &lt;a href=&#34;http://manuals.bioinformatics.ucr.edu/home/ht-seq#Introduction&#34;&gt;Bioconductor for analyzing high-throughput sequencing data files&lt;/a&gt;. Bioconductor hosts packages for other technologies/problems, so I would also look at it’s own help pages like the Workflows section. I also like &lt;a href=&#34;http://www2.warwick.ac.uk/fac/sci/moac/people/students/peter_cock/r&#34;&gt;Peter’s R programming pages&lt;/a&gt;, specially the heatmap section. I have my own share of Bioconductor related slides &lt;a href=&#34;http://www.biostat.jhsph.edu/~lcollado/teaching.html#.UDblsWie5Ng&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Learning LaTeX. I learnt the hard way I guess… I learnt by comparing Sweave files and their output and seeing what changed if I modified the code. Nowadays, I would very highly recommend that you first check the &lt;a href=&#34;http://www.uwlax.edu/faculty/matchett/late/late.htm&#34;&gt;How to Use LaTeX&lt;/a&gt; short series of exercises/files by Andrew Matchett. The &lt;a href=&#34;http://tobi.oetiker.ch/lshort/lshort.pdf&#34;&gt;Not so short introduction to LaTeX&lt;/a&gt; is a great resource. For very specific symbols, check the &lt;a href=&#34;http://www.ung.si/~sstanic/teaching/CIS/LaTeX_symbols-a4.pdf&#34;&gt;Comprehensive LaTeX symbol list&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Using a SGE (Sun Grid Engine) cluster. For some basic commands look &lt;a href=&#34;http://www.rcc.uh.edu/hpc-docs/49-using-torque-to-submit-and-monitor-jobs.html&#34;&gt;here&lt;/a&gt;. For the Hopkins cluster, definitely read &lt;a href=&#34;http://www.biostat.jhsph.edu/bit/cluster-usage.html&#34;&gt;this&lt;/a&gt;. Finally, for running array jobs check &lt;a href=&#34;https://wiki.duke.edu/display/SCSC/SGE+Array+Jobs&#34;&gt;this&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;LaTeX and math. You should definitely &lt;a href=&#34;http://en.wikibooks.org/wiki/LaTeX/Mathematics&#34;&gt;read the wiki books page for this topic&lt;/a&gt;. I kept going back to it over my first year at Hopkins when I really needed to learn all this. The &lt;a href=&#34;http://en.wikibooks.org/wiki/LaTeX/Theorems&#34;&gt;theorems page&lt;/a&gt; is nice, but not a must. Same for &lt;a href=&#34;http://www.stat.ubc.ca/~webmaste/howto/editor/texerr.html&#34;&gt;Common TeX/LaTeX errors&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Figures in LaTeX. &lt;a href=&#34;http://www.hep.manchester.ac.uk/u/jenny/jcwdocs/latex/figures.html&#34;&gt;Here is a basic overview &lt;/a&gt;but the &lt;a href=&#34;http://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions&#34;&gt;wiki books page for the topic&lt;/a&gt; is a must check.&lt;/li&gt;
&lt;li&gt;Accents in LaTeX. Check this &lt;a href=&#34;http://fontignie.blogspot.com/2006/04/accents-in-latex.html&#34;&gt;blog post&lt;/a&gt; by “Bugs and Solutions”.&lt;/li&gt;
&lt;li&gt;Blogging R code: &lt;a href=&#34;http://www.inside-r.org/pretty-r&#34;&gt;Pretty-R&lt;/a&gt;. I haven’t really used it but it surely looks pretty!!&lt;/li&gt;
&lt;li&gt;Cloud storage: &lt;a href=&#34;https://www.dropbox.com/&#34;&gt;Dropbox&lt;/a&gt; works great and tons of iPad apps have an option to backup to it which works great with my note-taking apps. &lt;a href=&#34;https://drive.google.com/&#34;&gt;Google Drive&lt;/a&gt; and others are also around.&lt;/li&gt;
&lt;li&gt;Paper (biobliography) organizer: &lt;a href=&#34;http://www.zotero.org/&#34;&gt;Zotero&lt;/a&gt; is amazing! I simply love it :) I pull the bibliography from pubmed or the magazine page itself and to avoid any hassle, I have a “papers” folder in my Dropbox where I only organize them by last name. Then if I want to find something, I go to Zotero and look use it’s great search function. I rarely use it to annotate webpages and I hear that it can now upload files to the cloud. Anyhow, I first used it in my Windows/Ubuntu dual setup. You can use it as a Zotero Firefox plugin or as Zotero stand-alone with Zotero Connector (Google Chrome for example). Finally, Zotero can export your bibliography into a BibTex file :)&lt;/li&gt;
&lt;li&gt;Blog: &lt;a href=&#34;http://www.tumblr.com/dashboard&#34;&gt;Tumblr&lt;/a&gt;. Some like WordPress better, but I like how Tumblr is not only a blogging platform but also a social media tool. I’ve written &lt;a href=&#34;http://fellgernon.tumblr.com/tagged/Blog#.UDbiDGie5Ng&#34;&gt;several posts before on how to customize your blog&lt;/a&gt; and other blog related tools. But a must in my point of view is to get your RSS feed “burnt” with &lt;a href=&#34;http://feedburner.google.com&#34;&gt;feedburner&lt;/a&gt;. It has lots of interesting tools and is much better than a plain XML RSS feed.&lt;/li&gt;
&lt;li&gt;Notes: &lt;a href=&#34;http://itunes.apple.com/us/app/notability-take-notes-annotate/id360593530?mt=8&#34;&gt;Notability&lt;/a&gt; iPad app. Great stuff and doesn’t blow up on you (aka, doesn’t lose your notes) like &lt;a href=&#34;http://notesplusapp.com/&#34;&gt;NotesPlus&lt;/a&gt; did to me. Anyhow, note-taking in my iPad with auto-cloud backups greatly changed my classroom experience. THere are other apps like this one out there.&lt;/li&gt;
&lt;li&gt;I use &lt;a href=&#34;http://itunes.apple.com/us/app/voice-recorder-hd/id373045717?mt=8&#34;&gt;Voice Recorder HD&lt;/a&gt; iPad app for recording lectures. It’s useful when you miss something the professor went over quickly and you are trying to understand it later on.&lt;/li&gt;
&lt;li&gt;Email: &lt;a href=&#34;http://www.gmail.com&#34;&gt;Gmail&lt;/a&gt; with keyboard shortcuts enabled. I also use “Canned Responses” from the Google Labs to specify my signature. I have an academic one, one for Mexico, etc.&lt;/li&gt;
&lt;li&gt;Send emails later: &lt;a href=&#34;http://www.boomeranggmail.com/&#34;&gt;Boomerang for Gmail&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Calendar: &lt;a href=&#34;https://www.google.com/calendar&#34;&gt;Google Calendar&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Task manager: Google Tasks from within Google Calendar (not from Gmail, which is doable too) with &lt;a href=&#34;http://itunes.apple.com/us/app/gotasks-google-tasks-client/id389113399?mt=8&#34;&gt;GoTasks&lt;/a&gt; in my iPhone.&lt;/li&gt;
&lt;li&gt;RSS reader: &lt;a href=&#34;http://www.google.com/reader/view/&#34;&gt;Google Reader&lt;/a&gt;. Works great. By the way, Orbvious interest (below) also works with Google Reader and has a customizable hotkey for it.&lt;/li&gt;
&lt;li&gt;Mark pages to read later: &lt;a href=&#34;http://shalom.craimer.org/projects/orbviousinterest/&#34;&gt;Orbvious Interest&lt;/a&gt; for Google Chrome. Great stuff! It syncs between computers and you can use Pocket in your iPad to view the links. &lt;/li&gt;
&lt;li&gt;Maps: &lt;a href=&#34;https://maps.google.com/&#34;&gt;Google Maps&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Video conversation: &lt;a&gt;Skype&lt;/a&gt;, Google Hangouts. If I’m going to help someone remotely, then I use &lt;a href=&#34;http://www.teamviewer.com/en/index.aspx&#34;&gt;TeamViewer&lt;/a&gt; which is free for non-profit purposes. With it you can move their mouse, which makes things much easier for support issues!&lt;/li&gt;
&lt;li&gt;Photos: &lt;a href=&#34;http://picasa.google.com/&#34;&gt;Picasa&lt;/a&gt;. I pay the 5 bucks a year for 20 GB on Picasa Web Albums so all my photos are on the cloud.&lt;/li&gt;
&lt;li&gt;Dictionary: &lt;a href=&#34;http://dictionary.die.net/&#34;&gt;die.net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Network visualizer/analyzer: &lt;a href=&#34;http://www.cytoscape.org/&#34;&gt;Cytoscape&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://bioinformatics.psb.ugent.be/webtools/Venn/&#34;&gt;Venn diagrams&lt;/a&gt; with more than 2 sets.&lt;/li&gt;
&lt;li&gt;Setting up your website. I pretty much followed Alyssa’s instructions and got my CSS template for &lt;a href=&#34;http://www.biostat.jhsph.edu/~lcollado/#.UDbpKmie5Ng&#34;&gt;my academic page&lt;/a&gt; from &lt;a href=&#34;http://www.freecsstemplates.org/&#34;&gt;FCT&lt;/a&gt;. Then I used simple html to modify it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I pretty much dumped a ton of my bookmarks in this huuuuge post! Well, I hope that it will be useful to someone. At least now I&amp;#8217;m happy to have contributed to Hilary&amp;#8217;s computing-resources-post drive.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
